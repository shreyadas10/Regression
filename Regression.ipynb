{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression ?\n",
        "\n",
        " -> Simple Linear Regression (SLR) is a statistical method that models the relationship between two continuous variables, where one variable (independent variable) is used to predict the other variable (dependent variable), with the goal of understanding the linear relationship and making predictions."
      ],
      "metadata": {
        "id": "l-mihRY4fY8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression ?\n",
        "\n",
        "->  The key assumptions of Simple Linear Regression (SLR) are:\n",
        "\n",
        "- Linearity: The relationship between the independent variable (predictor) and the dependent variable (response) should be linear.\n",
        "\n",
        "- Independence: Each observation should be independent of the others. This means that the data points should not be paired or matched in any way.\n",
        "\n",
        "- Homoscedasticity: The variance of the residuals (errors) should be constant across all levels of the independent variable.\n",
        "\n",
        "- Normality: The residuals should be normally distributed.\n",
        "\n",
        "- No Multicollinearity: The independent variable should not be highly correlated with other independent variables.\n",
        "\n",
        "-> These assumptions are necessary to ensure that the SLR model is valid and reliable, and that the results are interpretable."
      ],
      "metadata": {
        "id": "HqAHpSJagZda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "\n",
        "-> In the equation Y = mX + c, the coefficient \"m\" represents the slope of the linear relationship between X and Y.\n",
        "\n",
        "-> It measures the change in Y for a one-unit change in X. In other words, it represents the rate at which Y increases or decreases as X increases.\n",
        "\n",
        "-> A positive value of \"m\" indicates a positive relationship (as X increases, Y also increases), while a negative value of \"m\" indicates a negative relationship (as X increases, Y decreases)."
      ],
      "metadata": {
        "id": "lVb_aOk5hNXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "\n",
        "-? In the equation Y = mX + c, the intercept \"c\" represents the value of Y when X is equal to 0.\n",
        "\n",
        "-> It is the point at which the linear relationship crosses the Y-axis. In other words, it is the starting or reference point of the linear relationship."
      ],
      "metadata": {
        "id": "kHIEIEfkhNFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "\n",
        "-> The slope (m) in Simple Linear Regression can be calculated using the following formula:\n",
        "\n",
        "m = Σ[(xi - x̄)(yi - ȳ)] / Σ(xi - x̄)²\n",
        "\n",
        "Where:\n",
        "\n",
        "- m = slope\n",
        "- xi = individual data points of the independent variable (X)\n",
        "- x̄ = mean of the independent variable (X)\n",
        "- yi = individual data points of the dependent variable (Y)\n",
        "- ȳ = mean of the dependent variable (Y)\n",
        "- Σ = summation symbol, indicating the sum of the values"
      ],
      "metadata": {
        "id": "xlJd5nJ9hM2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "\n",
        "->To minimize the sum of the squared errors (residuals) between the observed data points and the predicted regression line.\n",
        "\n",
        "-> In other words, it finds the best-fitting line that minimizes the sum of the squared differences between the actual and predicted values of the dependent variable (Y)."
      ],
      "metadata": {
        "id": "P3ymQB8W_Wom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?\n",
        "\n",
        "-> The coefficient of determination (R²) in Simple Linear Regression measures the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (X).\n",
        "\n",
        "->   R² is interpreted as follows:\n",
        "\n",
        "- R² = 1: Perfect fit (all variance in Y is explained by X)\n",
        "- R² = 0: No fit (none of the variance in Y is explained by X)\n",
        "- 0 < R² < 1: Partial fit (some variance in Y is explained by X)\n",
        "\n",
        "-> In general:\n",
        "\n",
        "- High R² (close to 1): Strong relationship between X and Y\n",
        "- Low R² (close to 0): Weak relationship between X and Y"
      ],
      "metadata": {
        "id": "K16Ie2xXCDUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is Multiple Linear Regression ?\n",
        "\n",
        "-> Multiple Linear Regression (MLR) is a statistical technique that predicts the value of a continuous outcome variable (Y) based on two or more predictor variables (X1, X2, ..., Xn).\n",
        "\n",
        "-> In MLR, the relationship between the outcome variable and the predictor variables is modeled using a linear equation:\n",
        "\n",
        "Y = β0 + β1X1 + β2X2 + … + βnXn + ε\n",
        "\n",
        "Where:\n",
        "\n",
        "- Y = outcome variable\n",
        "- X1, X2, …, Xn = predictor variables\n",
        "- β0 = intercept or constant term\n",
        "- β1, β2, …, βn = coefficients of the predictor variables\n",
        "- ε = error term\n",
        "\n",
        "MLR is used to:\n",
        "\n",
        "- Examine the relationships between multiple predictor variables and an outcome variable\n",
        "- Control for the effects of multiple predictor variables on the outcome variable\n",
        "- Make predictions about the outcome variable based on the values of the predictor variables."
      ],
      "metadata": {
        "id": "zSt1NlbzCWDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "\n",
        "-> The main difference between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR) is:\n",
        "\n",
        "The number of predictor variables used in the model.\n",
        "\n",
        "-> Simple Linear Regression (SLR) uses:\n",
        "\n",
        "- One predictor variable (X) to predict the outcome variable (Y)\n",
        "\n",
        "-> Multiple Linear Regression (MLR) uses:\n",
        "\n",
        "- Two or more predictor variables (X1, X2, ..., Xn) to predict the outcome variable (Y)\n",
        "\n",
        "-> In other words, SLR examines the relationship between one independent variable and the dependent variable, while MLR examines the relationships between multiple independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "Upj3oihQC2-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "\n",
        "-> The key assumptions of Multiple Linear Regression (MLR) are:\n",
        "\n",
        "- Linearity: The relationship between each predictor variable and the outcome variable should be linear.\n",
        "\n",
        "- Independence: Each observation should be independent of the others.\n",
        "\n",
        "- Homoscedasticity: The variance of the residuals (errors) should be constant across all levels of the predictor variables.\n",
        "\n",
        "- Normality: The residuals should be normally distributed.\n",
        "\n",
        "- No Multicollinearity: The predictor variables should not be highly correlated with each other.\n",
        "\n",
        "- No Auto-correlation: The residuals should not be auto-correlated.\n",
        "\n",
        "These assumptions are necessary to ensure that the MLR model is valid and reliable, and that the results are interpretable."
      ],
      "metadata": {
        "id": "94MeASTODMyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
        "\n",
        "-> Heteroscedasticity, or non-constant error variance, means the variability of the error term in a regression model changes with the values of the independent variables, violating a key assumption of linear regression. This can lead to unreliable hypothesis tests and biased standard errors, even if the model's predictions remain unbiased.\n"
      ],
      "metadata": {
        "id": "sLisn2OibPry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "\n",
        "-> To improve a Multiple Linear Regression (MLR) model with high multicollinearity:\n",
        "\n",
        "- Remove correlated variables: Drop one of the highly correlated variables to reduce multicollinearity.\n",
        "- Regularization techniques: Use methods like Lasso or Ridge regression to reduce the impact of correlated variables.\n",
        "- Dimensionality reduction: Apply techniques like Principal Component Analysis (PCA) or Partial Least Squares (PLS) to reduce the number of variables.\n",
        "- Collect more data: Increasing the sample size can help alleviate multicollinearity issues.\n",
        "- Transform variables: Consider transforming variables to reduce correlation, such as logarithmic or standardization transformations."
      ],
      "metadata": {
        "id": "lU2da--Zdgfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "\n",
        "-> To prepare categorical variables for regression models, common techniques include one-hot encoding (creating binary columns for each category), label encoding (assigning numerical labels), and dummy coding (creating binary variables for all but one category)."
      ],
      "metadata": {
        "id": "edG3t1X6fLQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "\n",
        "-> Interaction terms in Multiple Linear Regression (MLR) allow  to model the relationship between two or more independent variables and the dependent variable.\n",
        "\n",
        "-> Role of interaction terms:\n",
        "\n",
        "1. Capture non-additive effects: Interaction terms enable you to model how the effect of one independent variable on the dependent variable changes depending on the level of another independent variable.\n",
        "2. Improve model fit: Including interaction terms can improve the model's explanatory power and reduce residual variance.\n",
        "3. Provide insights into relationships: Interaction terms can reveal complex relationships between variables, such as synergies or antagonisms.\n"
      ],
      "metadata": {
        "id": "1QsvfBkffxtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "-> In Simple Linear Regression (SLR), the intercept represents:\n",
        "\n",
        "- The expected value of the dependent variable when the independent variable is equal to zero.\n",
        "\n",
        "In Multiple Linear Regression (MLR), the intercept represents:\n",
        "\n",
        "- The expected value of the dependent variable when all independent variables are equal to zero.\n",
        "\n",
        "-> Key differences:\n",
        "\n",
        "- SLR: Intercept is the expected value when the single independent variable is zero.\n",
        "- MLR: Intercept is the expected value when all independent variables are zero.\n",
        "\n",
        "-> This difference is crucial for accurate interpretation, especially when independent variables have different scales or units."
      ],
      "metadata": {
        "id": "lq56hjnYgDiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "\n",
        "-> The slope in regression analysis represents the change in the dependent variable for a one-unit change in the independent variable, while holding all other independent variables constant.\n",
        "\n",
        "Significance of the slope:\n",
        "\n",
        "- Direction of relationship: The sign of the slope indicates the direction of the relationship between the variables.\n",
        "- Magnitude of change: The magnitude of the slope represents the amount of change in the dependent variable for a one-unit change in the independent variable.\n",
        "- Prediction: The slope is used to make predictions by multiplying it with the value of the independent variable.\n",
        "\n",
        "-> Effect on predictions:\n",
        "\n",
        "- Steep slope: A steep slope indicates a large change in the dependent variable for a small change in the independent variable, resulting in more extreme predictions.\n",
        "- Shallow slope: A shallow slope indicates a small change in the dependent variable for a large change in the independent variable, resulting in more moderate predictions.\n",
        "- Zero slope: A zero slope indicates no change in the dependent variable for a change in the independent variable, resulting in predictions that are unaffected by the independent variable."
      ],
      "metadata": {
        "id": "Nryre2y_gmWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "\n",
        "-> The intercept in a regression model:\n",
        "\n",
        "- Provides a baseline value: The intercept represents the expected value of the dependent variable when all independent variables are equal to zero.\n",
        "- Contextualizes the relationship: The intercept helps understand the starting point of the relationship between the variables, providing context for the slope.\n",
        "- Indicates the constant effect: The intercept represents the constant effect on the dependent variable, regardless of the values of the independent variables."
      ],
      "metadata": {
        "id": "RFR06GyDg5ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "\n",
        "-> R² (coefficient of determination) has limitations as a sole measure of model performance:\n",
        "\n",
        "- Overfitting: High R² values can be achieved with overfitting models that perform poorly on new data.\n",
        "- Model complexity: R² increases with added variables, regardless of their relevance, making it biased towards complex models.\n",
        "- Non-normal residuals: R² assumes normal residuals; non-normality can lead to inaccurate R² values.\n",
        "- Outliers and influential points: R² is sensitive to outliers and influential points, which can artificially inflate or deflate its value.\n",
        "- Lack of predictive power: High R² values do not guarantee good predictive performance, especially with non-linear relationships.\n",
        "\n"
      ],
      "metadata": {
        "id": "SOBwhGEhhIk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient ?\n",
        "\n",
        "-> A large standard error for a regression coefficient indicates:\n",
        "\n",
        "- High variability: The coefficient estimate is highly variable, making it less reliable.\n",
        "- Low precision: The estimate is not precise, indicating a wide range of possible values.\n",
        "- Potential insignificance: A large standard error can lead to a non-significant p-value, suggesting the variable may not be a significant predictor.\n",
        "\n",
        "-> Possible causes:\n",
        "\n",
        "- Multicollinearity: Correlation between independent variables can inflate standard errors.\n",
        "- Small sample size: Limited data can lead to large standard errors.\n",
        "- Outliers or influential points: Data points with high leverage can increase standard errors."
      ],
      "metadata": {
        "id": "GSVzx9YbhmuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "\n",
        "-> Heteroscedasticity can be identified in residual plots by looking for:\n",
        "\n",
        "- Non-random scatter: Residuals are not randomly scattered around the horizontal axis.\n",
        "- Fanning or tapering: Residuals spread out or narrow down as the fitted values increase.\n",
        "- Non-constant variance: Residuals have varying levels of dispersion across different ranges of fitted values.\n",
        "\n",
        "-> Addressing heteroscedasticity is important because:\n",
        "\n",
        "- Inaccurate inference: Heteroscedasticity can lead to incorrect conclusions about the significance of regression coefficients.\n",
        "- Poor predictions: Ignoring heteroscedasticity can result in suboptimal predictions and inaccurate confidence intervals.\n",
        "- Model misspecification: Heteroscedasticity can indicate a misspecified model, which can be improved by addressing the issue."
      ],
      "metadata": {
        "id": "BuLax1eoh1g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
        "\n",
        "-> If a Multiple Linear Regression (MLR) model has:\n",
        "\n",
        "- High R²: The model explains a large proportion of the variance in the dependent variable.\n",
        "- Low adjusted R²: The model's explanatory power is largely due to the number of predictors, rather than their actual relationship with the dependent variable.\n",
        "\n",
        "This discrepancy suggests:\n",
        "\n",
        "1. Overfitting: The model is too complex and fits the noise in the training data.\n",
        "2. Multicollinearity: Correlation between predictors inflates R² but reduces adjusted R²."
      ],
      "metadata": {
        "id": "mm5rqsm6iG4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "\n",
        "-> Scaling variables in Multiple Linear Regression (MLR) is important for:\n",
        "\n",
        "- Preventing feature dominance: Scaling ensures that variables with large ranges don't dominate the model, allowing variables with smaller ranges to contribute equally.\n",
        "- Improving model interpretability: Scaled variables enable more intuitive interpretation of regression coefficients, as they represent changes in standardized units.\n",
        "- Enhancing model stability: Scaling reduces the impact of extreme values and outliers, leading to more stable model estimates.\n",
        "- Facilitating regularization: Scaling is essential for regularization techniques like Lasso and Ridge regression, which rely on standardized variables to apply penalties evenly."
      ],
      "metadata": {
        "id": "scVylDz54IrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "-> Polynomial regression is a type of regression analysis that models the relationship between an independent variable and a dependent variable using an nth-degree polynomial, allowing for non-linear relationships that linear regression cannot capture."
      ],
      "metadata": {
        "id": "TeF0FOvM4cL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression ?\n",
        "\n",
        "-> Polynomial regression and linear regression differ in several key ways:\n",
        "\n",
        "-> Relationship modeling:\n",
        "\n",
        "- Linear regression: Models a linear relationship between independent variables and the dependent variable.\n",
        "- Polynomial regression: Models a non-linear relationship using a polynomial equation.\n",
        "\n",
        "-> Complexity:\n",
        "\n",
        "- Linear regression: Assumes a simple, linear relationship.\n",
        "- Polynomial regression: Can model more complex, non-linear relationships.\n",
        "\n",
        "-> Interpretation:\n",
        "\n",
        "- Linear regression: Coefficients represent changes in the dependent variable for a one-unit change in the independent variable.\n",
        "- Polynomial regression: Coefficients are more difficult to interpret due to the non-linear relationship.\n",
        "\n",
        "-> Flexibility :\n",
        "\n",
        "- Linear regression: Limited to modeling linear relationships.\n",
        "- Polynomial regression: Can model a wide range of non-linear relationships.\n",
        "\n",
        "->  Risk of overfitting:\n",
        "\n",
        "- Linear regression: Less prone to overfitting due to its simplicity.\n",
        "- Polynomial regression: Higher risk of overfitting, especially with high-degree polynomials."
      ],
      "metadata": {
        "id": "IzdrUM-U4wh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  When is polynomial regression used ?\n",
        "\n",
        "-> Polynomial regression is used in various scenarios:\n",
        "\n",
        "- Non-linear relationships: When the relationship between variables is non-linear, polynomial regression can capture these complex relationships.\n",
        "- Curve fitting: Polynomial regression is often used for curve fitting, especially when the underlying relationship is smooth and continuous.\n",
        "- Data modeling: In data modeling, polynomial regression can be used to model complex systems, such as population growth or chemical reactions.\n",
        "- Time series forecasting: Polynomial regression can be used to model and forecast time series data, especially when there are non-linear trends.\n",
        "- Signal processing: In signal processing, polynomial regression can be used to model and analyze signals, such as audio or image signals."
      ],
      "metadata": {
        "id": "82yhX6IQBc8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  What is the general equation for polynomial regression ?\n",
        "\n",
        "->The general equation for polynomial regression is:-  \n",
        " - y = β₀ + β₁x + β₂x² + ... + βₙxⁿ\n",
        " - 'y' is the dependent variable, 'x' is the independent variable, 'β₀' to 'βₙ' are coefficients, and 'n' is the degree of the polynomial."
      ],
      "metadata": {
        "id": "st8grtthDQNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables ?\n",
        "-> Yes, polynomial regression can be applied to multiple variables. This is known as multivariate polynomial regression.\n",
        "\n",
        "Key aspects:\n",
        "\n",
        "- Multiple independent variables: The model includes multiple independent variables (x₁, x₂, ..., xₘ) that are used to predict the dependent variable (y).\n",
        "- Polynomial terms: The model includes polynomial terms for each independent variable, such as x₁², x₂³, etc.\n",
        "- Interaction terms: The model can also include interaction terms between independent variables, such as x₁x₂, x₁²x₂, etc."
      ],
      "metadata": {
        "id": "7PwH5thpFZQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression ?\n",
        "\n",
        "-> Polynomial regression has several limitations:\n",
        "\n",
        "- Overfitting: Polynomial regression models can easily overfit the training data, especially when using high-degree polynomials.\n",
        "- Complexity: Polynomial regression models can become overly complex, making them difficult to interpret and prone to overfitting.\n",
        "- Non-interpretable coefficients: The coefficients of polynomial regression models can be difficult to interpret, especially when using high-degree polynomials.\n",
        "- Sensitive to outliers: Polynomial regression models can be sensitive to outliers and influential points, which can affect the accuracy of the model.\n",
        "- Not suitable for binary or categorical outcomes: Polynomial regression is not suitable for binary or categorical outcomes, as it is designed for continuous outcomes.\n",
        "- Computationally expensive: Polynomial regression models can be computationally expensive to estimate, especially when using large datasets or high-degree polynomials.\n",
        "- Difficulty in selecting the degree of the polynomial: Selecting the optimal degree of the polynomial can be challenging, and using a degree that is too high or too low can affect the accuracy of the model.\n"
      ],
      "metadata": {
        "id": "bZmv2r2lFtFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "\n",
        "-> When selecting the degree of a polynomial, several methods can be used to evaluate model fit:\n",
        "\n",
        "- Visual inspection: Plot the data and the fitted polynomial to visually assess the fit.\n",
        "- Coefficient of determination (R²): Calculate R² to measure the proportion of variance explained by the model.\n",
        "- Mean squared error (MSE): Calculate MSE to measure the average squared difference between predicted and actual values.\n",
        "- Mean absolute error (MAE): Calculate MAE to measure the average absolute difference between predicted and actual values.\n"
      ],
      "metadata": {
        "id": "9OMxx9geGGcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression ?\n",
        "\n",
        "-> Visualization is essential in polynomial regression for several reasons:\n",
        "\n",
        "- Understanding relationships: Visualization helps to understand the relationships between variables, including non-linear relationships.\n",
        "- Checking assumptions: Visualization can be used to check assumptions of polynomial regression, such as linearity, homoscedasticity, and normality.\n",
        "- Model selection: Visualization can aid in selecting the optimal degree of the polynomial by comparing the fit of different models.\n",
        "- Identifying outliers: Visualization can help identify outliers and influential points that may affect the model's performance.\n",
        "- Communicating results: Visualization can be used to effectively communicate the results of the polynomial regression analysis to stakeholders.\n"
      ],
      "metadata": {
        "id": "4e7ueKo1HKWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.  How is polynomial regression implemented in Python?\n",
        "\n",
        "-> Polynomial regression can be implemented in Python using several libraries, including:\n",
        "\n",
        "- NumPy: For numerical computations.\n",
        "- SciPy: For scientific computing.\n",
        "- Scikit-learn: For machine learning."
      ],
      "metadata": {
        "id": "CVyPtFlVHbnI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CGKHCz7CCoX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}